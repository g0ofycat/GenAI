# Transformer Model (AI)

A full Transformer model built in Python (including both encoding and decoding). Side project.

## Architecture & Design Choices

- **[GPT-2's Merge & Vocabulary Files](https://huggingface.co/openai-community/gpt2/tree/main)**
- **Default Parameter Count (When Trained):** *~15 M*
- **Fully Customizable Transformer Settings**
- **Chatbot class for Inference + Foreign Character Cleanup**
- **Multiple Sampling Strategies & Tokenization Methods**
- **GeLU Activation for Both FFN Layers**

## TODO 

- [x] Create a full Transformer
- [x] Revise & Use proper Weight & Bias initialization before I train it
- [x] Polish